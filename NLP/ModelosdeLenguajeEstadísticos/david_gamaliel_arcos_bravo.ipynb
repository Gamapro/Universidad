{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David Gamaliel Arcos Bravo\n",
    "### T3: Modelos de Lenguaje Estadísticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5544,), (5544,))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_files(PATH_TRAIN, PATH_TRAIN_LABELS):\n",
    "    train, train_labels = [], []\n",
    "    with open(PATH_TRAIN, 'r', encoding=\"utf8\") as f:\n",
    "        train = [ line for line in f]\n",
    "    with open(PATH_TRAIN_LABELS, 'r', encoding=\"utf8\") as f:\n",
    "        train_labels = f.readlines()\n",
    "    return np.array(train), np.array(train_labels)\n",
    "\n",
    "PATH_TRAIN = '../DatasetAgresividad/mex_train.txt'\n",
    "PATH_LABELS = '../DatasetAgresividad/mex_train_labels.txt'\n",
    "tr_txt, tr_y = get_files(PATH_TRAIN, PATH_LABELS)\n",
    "tr_txt.shape, tr_y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (5pts) Preprocese todos los tuits de agresividad (positivos y negativos) según su intu-\n",
    "ición para construir un buen corpus para un modelo de lenguaje (e.g., solo palabras en minúscula, etc.). Agregue tokens especiales de <s> y </s> según usted considere (e.g., al\n",
    "inicio y final de cada tuit). Defina su vocabulario y enmascare con <unk> toda palabra\n",
    "que no esté en su vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fdist(corpus_words, filter = False):\n",
    "    fdist = nltk.FreqDist(corpus_words)\n",
    "    aux = [(fdist[key], key) for key in fdist]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    if filter == True: \n",
    "        fdist = aux[:1000]\n",
    "    else:\n",
    "        fdist = aux\n",
    "    fdist = dict([(word, freq) for freq, word in fdist])\n",
    "    fdist_idx = dict([(word, i) for i, word in enumerate(fdist)])\n",
    "    return fdist, fdist_idx\n",
    "\n",
    "def get_corpus(tweets, vocab):\n",
    "    corpus_words = []\n",
    "    for txt in tweets:\n",
    "        for word in txt:\n",
    "            if word in vocab:\n",
    "                corpus_words.append(word)\n",
    "    return corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(tr_txt):\n",
    "\n",
    "    # Tokenizar y agregar <s> y </s>\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "    tr_txt_tknzd = [['<s>'] + tknzr.tokenize(tuit) + ['</s>'] for tuit in tr_txt]\n",
    "    \n",
    "    # Crear vocabulario\n",
    "\n",
    "    vocab = set()\n",
    "    for tuit in tr_txt_tknzd:\n",
    "        for word in tuit:\n",
    "            if word.isalpha() or word in ['<s>', '</s>']:\n",
    "                vocab.add(word)\n",
    "\n",
    "    # Crear corpus como lista y calcular fdist filtrando\n",
    "\n",
    "    corpus_words = get_corpus(tr_txt_tknzd, vocab)\n",
    "    fdist, fdist_idx = calculate_fdist(corpus_words, filter = True)\n",
    "    \n",
    "    # Generar vocabulario nuevo con palabras filtradas\n",
    "\n",
    "    vocab = set(fdist.keys())\n",
    "    vocab.add('<unk>')\n",
    "    vocab.add('<s>')\n",
    "    vocab.add('</s>')\n",
    "\n",
    "    # Retokenizar y cambiar valores de vocab por unk\n",
    "\n",
    "    tr_txt_tknzd = [[word if word in vocab else '<unk>' for word in tuit] for tuit in tr_txt_tknzd]\n",
    "\n",
    "    # Generar nuevo corpus y fdist\n",
    "    corpus_words = get_corpus(tr_txt_tknzd, vocab)\n",
    "    fdist, fdist_idx = calculate_fdist(corpus_words)\n",
    "\n",
    "    return tr_txt_tknzd, vocab, fdist, fdist_idx\n",
    "\n",
    "tr_txt_tknzd, vocab, fdist, fdist_idx = process_corpus(tr_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<unk>', '<unk>', 'putos', '<unk>', '<unk>', 'los', '<unk>', 'minutos', 'que', 'se', 'hacen', 'de', 'mi', 'casa', 'a', 'mi', '<unk>', '<unk>', '<unk>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tr_txt_tknzd[np.random.randint(len(tr_txt_tknzd))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el procesamiento primero tokenize los tweets para agregar los caracteres especiales, filtre caracteres no validos y saque distribuciones, Despues repeti el proceso para tener un corpus mas estable en palabras importante y sin caracteres innecesarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (20pts) Entrene tres modelos de lenguaje sobre todos los tuits: unigramas, bigrama , trigramas\n",
    "Para cada uno proporcione una interfaz (función) sencilla \n",
    "Los modelos deben tener una estrategia común para lidiar con secuencias no vistas. \n",
    "Puede optar por un suavizamiento Laplace o un Good-Turing\n",
    "discounting. Muestre un par de ejemplos de como funciona, al menos uno con una palabra fuera del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'pendejo', 'pendejo', 'traen', 'tráfico', 'cree', 'escucha', 'pendejo', 'vídeo', 'pendejo', 'viejas', 'pendejo', 'puñal', 'cierto', 'neta', 'vi', 'seguir', 'debería', 'ellas', 'va']\n"
     ]
    }
   ],
   "source": [
    "def get_unigram_counts(vocab, tweets):\n",
    "    # Numpy array\n",
    "    unigram_counts = np.zeros(len(vocab), dtype=np.float64)\n",
    "    for tuit in tweets:\n",
    "        for word in tuit:\n",
    "            unigram_counts[fdist_idx[word]] += 1\n",
    "    unigram_counts = unigram_counts / unigram_counts.sum()\n",
    "    return unigram_counts\n",
    "\n",
    "unigram_counts = get_unigram_counts(vocab, tr_txt_tknzd)\n",
    "\n",
    "def generate_sentence(unigram_counts, vocab, max_len=20):\n",
    "    sentence = ['<s>']\n",
    "    while len(sentence) < max_len:\n",
    "        word = np.random.choice(list(vocab), p=unigram_counts)\n",
    "        if word == '<s>':\n",
    "            continue\n",
    "        sentence.append(word)\n",
    "        if word == '</s>':\n",
    "            break\n",
    "    return sentence\n",
    "\n",
    "print(generate_sentence(unigram_counts, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'tenía', 'mía', 'tipo', 'ciudad', 'loca', 'como', 'cada', 'méxico', 'decirte', 'jaja', 'pensé', 'escucha', 'metro', 'jugadores', 'bendición', 'enfermo', 'maricones', 'chido', 'buenas']\n"
     ]
    }
   ],
   "source": [
    "def get_bigram_counts(fdist, fdist_idx, tweets):\n",
    "    bigram_counts = np.zeros(\n",
    "        (len(fdist), len(fdist)),\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    for tuit in tweets:\n",
    "        for i in range(len(tuit) - 1):\n",
    "            if tuit[i] in vocab and tuit[i + 1] in vocab:\n",
    "                bigram_counts[fdist_idx[tuit[i]], fdist_idx[tuit[i + 1]]] += 1\n",
    "    for i in range(len(fdist)):\n",
    "        total = np.sum(bigram_counts[i]) + len(fdist)\n",
    "        bigram_counts[i] = (bigram_counts[i] + 1) / total\n",
    "\n",
    "    return bigram_counts\n",
    "\n",
    "bigram_counts = get_bigram_counts(fdist, fdist_idx, tr_txt_tknzd)\n",
    "# print(bigram_counts)\n",
    "\n",
    "def generate_sentence_bigram(bigram_counts, fdist, fdist_idx, vocab, max_len=20):\n",
    "    sentence = ['<s>']\n",
    "    while len(sentence) < max_len:\n",
    "        if sentence[-1] == '</s>':\n",
    "            break\n",
    "        word = np.random.choice(list(vocab), p=bigram_counts[fdist_idx[sentence[-1]]])\n",
    "        sentence.append(word)\n",
    "    return sentence\n",
    "\n",
    "print(generate_sentence_bigram(bigram_counts, fdist, fdist_idx, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 1001, 1001)\n",
      "['<s>', '<s>', 'eres', 'ella', 'unos', 'cree', 'normal', 'creer', 'chivas', 'todavía', 'sabes', 'desde', 'caliente', 'comprar', 'osorio', 'porqué', 'espero', 'matar', 'maldito', 'saludos']\n"
     ]
    }
   ],
   "source": [
    "# Trigrama\n",
    "\n",
    "def get_trigram_counts(fdist, fdist_idx, tweets):\n",
    "    trigram_counts = np.ones(\n",
    "        (len(fdist), len(fdist), len(fdist)),\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    for tuit in tweets:\n",
    "        for i in range(len(tuit) - 2):\n",
    "            if tuit[i] in vocab and tuit[i + 1] in vocab and tuit[i + 2] in vocab:\n",
    "                trigram_counts[fdist_idx[tuit[i]], fdist_idx[tuit[i + 1]], fdist_idx[tuit[i + 2]]] += 1\n",
    "    for i in range(len(fdist)):\n",
    "        for j in range(len(fdist)):\n",
    "            total = np.sum(trigram_counts[i, j]) + len(fdist)\n",
    "            trigram_counts[i, j] = (trigram_counts[i, j] + 1) / total\n",
    "\n",
    "    return trigram_counts\n",
    "\n",
    "trigram_counts = get_trigram_counts(fdist, fdist_idx, tr_txt_tknzd)\n",
    "\n",
    "def generate_sentence_trigram(trigram_counts, fdist, fdist_idx, vocab, max_len=20):\n",
    "    sentence = ['<s>', '<s>']\n",
    "    while len(sentence) < max_len:\n",
    "        if sentence[-1] == '</s>':\n",
    "            break\n",
    "        word = np.random.choice(list(vocab), p=trigram_counts[fdist_idx[sentence[-2]], fdist_idx[sentence[-1]]])\n",
    "        sentence.append(word)\n",
    "    return sentence\n",
    "\n",
    "print(trigram_counts.shape)\n",
    "print(generate_sentence_trigram(trigram_counts, fdist, fdist_idx, vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte la principal tarea es correr un modelo con suficientes valores para poder generar sentencias con sentido, donde la memoria es importante de priorizar. Decidi filtrar sobre 2000 para tener un rendimiento estable con todos los modelos sin perder contexto ni palabras y de formas que corrieran de manera eficiente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (25pts) Construya un modelo interpolado con valores λ fijos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'esperando', 'triste', 'hermano', 'pendejo', 'xd', 'leche', 'final', 'rica', 'ratas', 'saliendo', 'traen', 'voy', 'esas', 'esperando', 'va', 'pendejo', 'caer', 'ojalá', 'siempre', 'ser', 'vídeo', 'pendejo', 'dando', 'boca', 'fotos', 'sales', 'risa', 'usar', 'pendejo', 'puerta', 'ustedes', 'feliz', 'llena', 'pendejo', 'vecinos', 'fea', 'hablan', 'acaba', 'enfermo', 'seguir', 'juego', 'usa', 'pendejo', 'like', 'estuvo', 'estudiar', 'viejas', 'nueva']\n"
     ]
    }
   ],
   "source": [
    "deltas = [0.2, 0.3, 0.5]\n",
    "\n",
    "def generate_sentence_interpolation(trigram_counts, bigram_counts, unigram_counts, fdist, fdist_idx, vocab, deltas, max_len=50):\n",
    "    sentence = ['<s>', '<s>']\n",
    "    while len(sentence) < max_len:\n",
    "        if sentence[-1] == '</s>':\n",
    "            break\n",
    "        word = np.random.choice(\n",
    "            list(vocab), \n",
    "            p = deltas[0] * trigram_counts[fdist_idx[sentence[-2]], fdist_idx[sentence[-1]]] +\n",
    "                deltas[1] * bigram_counts[fdist_idx[sentence[-1]]] +\n",
    "                deltas[2] * unigram_counts\n",
    "        )\n",
    "        sentence.append(word)\n",
    "    return sentence\n",
    "\n",
    "print(generate_sentence_interpolation(trigram_counts, bigram_counts, unigram_counts, fdist, fdist_idx, vocab, deltas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (20pts) Haga una función \"tuitear\" con base en su modelo de lenguaje P̂ del último punto.\n",
    "El modelo deberá poder parar automáticamente cuando genere el símbolo de terminación\n",
    "de tuit al final (e.g., \"</s>\"), o 50 palabras. Proponga algo para que en los últimos tokens\n",
    "sea más probable generar el token \"</s>\". Muestre al menos cinco ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> falta besos conmigo pendejo vuelven estudiar siempre porqué pendejo valga fotos asi palabras otros extraño televisa madres pinche pendejo frente estaría whatsapp escribir hicieron familia escuchando hdp primero jamás pendejo fue boca imagen saben vídeo pendejo tetas besos quienes ve gustas ten pendejo pierda tareas asi ponerme mal\n",
      "<s> <s> mandarte idea pendejo asi vagina nombre darte siente tetas bye hablan puros pendejo boca naturaleza puedo cierto américa fotos pendejo fotos cachonda vean seguir uno sido matar pendejo perro entre pendejo misma salió clase mismos haber pendejo pendejo hecho traen nueva conmigo gritando fuera espero facebook esperando juego\n",
      "<s> <s> saben like cierto estuvo pendejo pendejo traen pendeja carajo amor vídeo padres estás cierto sus pinche wey comentarios recuerdo gusta pendejo vi mamar visto manda toca pendejo nunca pendejo una cuando urge pinche cachetada cree imagino tengo traen tenía pendejo pendejo chica clase seguir seguir pendejo hoy super\n",
      "<s> <s> esté nada hecho siente ve tuit seguir super osorio fotos pendejo pendejo super quiero empieza pendejo llena cuál valgo tetas pendejo cierto oh q am pendejo extraño seguidores llena dar sabes opinión holanda eres hacen pedazo primer poder o quiero cuál había otras vales mexicanos deberían siento tetas\n",
      "<s> <s> algo cosa chingue mandan super dejan profe debería muy asco sabes placer pendejo quiere primer tienen berga alguien decirle otros ojala tráfico esperando juego pasa seguir mano salió he tetas saben pendejo váyanse celular traen pendejo vas extraño quedó zona temprano parecer pendejo deberían mismos pendejo buscando ahi\n"
     ]
    }
   ],
   "source": [
    "deltas = [0.2, 0.35, 0.45]\n",
    "\n",
    "def tuitear():\n",
    "    return ' '.join(generate_sentence_interpolation(trigram_counts, bigram_counts, unigram_counts, fdist, fdist_idx, vocab, deltas))\n",
    "\n",
    "for i in range(5):\n",
    "    print(tuitear())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (10pts) Use la intuición que ha ganado en esta tarea y los datos de las mañaneras para\n",
    "entrenar un modelo de lenguaje AMLO. Haga una un función \"dar_conferencia()\". Generé\n",
    "un discurso de 300 palabras y detenga al modelo de forma abrupta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> saben hizo trabajo relevante subsecretaria www Jesús proceso relevante problema hizo respecto mexicanos bajan principio principio hemos en relevante crean Para saben Ahora atendiendo siquiera autobuses diera principio pública estaban relevante Procedimientos cada puede da días pero casi principio tanta tenido estaban nacionalidad relevante correspondiente policía grave tercos procede justamente dio va relevante Cuarta legales transaccional integrante queremos seguir sigue León dato teníamos duda venganza relevante Sala Proyecto relevante legisladores servidor relevante agentes Código vemos favor importante Rayos corruptas Agustín relevante oficiosa parte conducta estaban Yo estenográfica Tamaulipas tanto ha plana relevante relevante Migración orden enero verificando explicara eso intuición saber cierto aplicar utilizado Antes juez Alejandro LUIS ha denuncias seguir relevante Qué Código trata señor relevante tienen pasados ese sumados está Tamaulipas seguir investigaciones relevante revisión relevante Instituto tratamos algunos relevante pública apoyo senadores cosas hoy dinero incluso federativas congresos patrulla sentido último Populismo suficiente terminar t voluntad presentadas verificando relevante Nos tiene propósito Bienestar relevante obtenidos siendo mañana hacer se carga Alfonso mañana suficiente elementos hace fueron suspensión medio gustaría política válvula tendrían Alfonso violación principio suficiente PRESIDENTE saben legal Ejecutivo relevante saben terreno tiene día Qué estaban sentido relevante relevante juntos simulación toda publicista seguir principio carga provenientes sino acaba secretaria seguridad Instituto servidores enero relevante Ley Cómo relevante reformas persona por Este LA tenían documental víctima estado perdón Agustín política seis migratorios delincuencia Versión mexicano va seguir seguramente cierto Ayotzinapa millones principio pesos punto ni administrativos secundaria ciudadanos semanas Ah planteamiento integrante cierto tuvieron virtud Por tipos seguir identificaciones diputados Bueno conocimiento foto relevante DE trabajo Santiago operativo presidentes lado unánime pueden menos artículo Entonces nueva electorales integrante propia Primero audiencia sindicalizados hacer nueva general estaban enero válvula wordpress pública área principio oficiosa SÁNCHEZ presidentes establece contemplan <s> será Es patria dato relevante dato\n"
     ]
    }
   ],
   "source": [
    "PATH_TRAIN = '../DatasetDownloadTarea/estenograficas_limpias_por_fecha/2019-03-14'\n",
    "PATH_LABELS = '../DatasetDownloadTarea/estenograficas_limpias_por_fecha/2019-05-12'\n",
    "\n",
    "amlo_tr_txt, amlo_tr_y = get_files(PATH_TRAIN, PATH_LABELS)\n",
    "amlo_tr_txt_tknzd, amlo_vocab, amlo_fdist, amlo_fdist_idx = process_corpus(tr_txt)\n",
    "\n",
    "amlo_unigram_counts = get_unigram_counts(amlo_vocab, amlo_tr_txt_tknzd)\n",
    "amlo_bigram_counts = get_bigram_counts(amlo_fdist, amlo_fdist_idx, amlo_tr_txt_tknzd)\n",
    "amlo_trigram_counts = get_trigram_counts(amlo_fdist, amlo_fdist_idx, amlo_tr_txt_tknzd)\n",
    "\n",
    "amlo_deltas = [0.2, 0.35, 0.45]\n",
    "\n",
    "def dar_conferencia(unigram_counts, bigram_counts, trigram_counts, fdist_idx, vocab, deltas, stop_words = 300):\n",
    "    conferencia = ['<s>', '<s>']\n",
    "    while len(conferencia) < stop_words:\n",
    "        word = np.random.choice(\n",
    "            list(vocab), \n",
    "            p = deltas[0] * trigram_counts[fdist_idx[conferencia[-2]], fdist_idx[conferencia[-1]]] +\n",
    "                deltas[1] * bigram_counts[fdist_idx[conferencia[-1]]] +\n",
    "                deltas[2] * unigram_counts\n",
    "        )\n",
    "        if word == '</s>':\n",
    "            continue\n",
    "        conferencia.append(word)\n",
    "    return ' '.join(conferencia)\n",
    "\n",
    "print(dar_conferencia(amlo_unigram_counts, amlo_bigram_counts, amlo_trigram_counts, amlo_fdist_idx, amlo_vocab, amlo_deltas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada la naturaleza de los modelos fue imposible generarlo para todos los archivos, puesto que estos pesaban mucho y tardaba demasiado en cargar, ademas que los reucrsos en memoria no daban por lo que decidi hacerlo con un archivo de las mananeras especialmente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (10pts) Calcule el estimado de cada uno sus modelos de lenguaje (el de tuits y el de amlo)\n",
    "para las frases: \"sino gano me voy a la chingada\", \"ya se va a acabar la corrupción\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.726317341747217e-09\n",
      "9.726317341747217e-09\n"
     ]
    }
   ],
   "source": [
    "def estimate_sentence_probability(sentence, trigram_counts, bigram_counts, unigram_counts, fdist, fdist_idx, deltas):\n",
    "    sentence = ['<s>', '<s>'] + sentence + ['</s>']\n",
    "    log_prob = 0\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word not in vocab:\n",
    "            sentence[i] = '<unk>'\n",
    "    for i in range(2, len(sentence)):\n",
    "        log_prob += np.log(\n",
    "            deltas[0] * trigram_counts[fdist_idx[sentence[-2]], fdist_idx[sentence[-1]]] +\n",
    "            deltas[1] * bigram_counts[fdist_idx[sentence[-1]]] +\n",
    "            deltas[2] * unigram_counts\n",
    "        )\n",
    "    # print( \"Probability of \" + ''.join(sentence) + \" is \" + str(np.exp(log_prob)))\n",
    "    return np.sum(np.exp(log_prob))\n",
    "\n",
    "print(estimate_sentence_probability(['sino', 'gano', 'me', 'voy', 'a', 'la', 'chingada'], trigram_counts, bigram_counts, unigram_counts, fdist, fdist_idx, deltas))\n",
    "print(estimate_sentence_probability(['ya', 'se', 'va', 'a', 'acabar', 'la', 'corrupción'], amlo_trigram_counts, amlo_bigram_counts, amlo_unigram_counts, amlo_fdist, amlo_fdist_idx, amlo_deltas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un punto importante a destacar en este modelo es que algunas palabras como chingada no estaban en el corpus original por lo que tuve que parsear estos valores por unk, y es interesante como las probabilidades son tan bjaas que incluso son iguales con modelos diferentes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (10pts) Para cada oración del punto anterior, haga todas las permutaciones posibles.\n",
    "Calcule su probabilidad a cada nueva frase y muestre el top 3 mas probable y el top 3\n",
    "menos probable (para ambos modelos de lenguaje). Proponga una frase más y haga lo\n",
    "mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sino', 'gano', 'me', 'voy', 'a', 'chingada', 'la'], ['sino', 'la', 'me', 'voy', 'a', 'gano', 'chingada'], ['voy', 'gano', 'me', 'sino', 'a', 'la', 'chingada']]\n",
      "[['ya', 'se', 'va', 'a', 'acabar', 'corrupción', 'la'], ['ya', 'la', 'va', 'a', 'acabar', 'se', 'corrupción'], ['a', 'se', 'va', 'ya', 'acabar', 'la', 'corrupción']]\n"
     ]
    }
   ],
   "source": [
    "def get_permutations(sentence):\n",
    "    permutations = []\n",
    "    for i in range(len(sentence)):\n",
    "        for j in range(len(sentence)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            permutation = sentence.copy()\n",
    "            permutation[i], permutation[j] = permutation[j], permutation[i]\n",
    "            permutations.append(permutation)\n",
    "    return permutations\n",
    "\n",
    "def get_top_n_permutations(sentence, n, trigram_counts, bigram_counts, unigram_counts, fdist, fdist_idx, deltas):\n",
    "    permutations = get_permutations(sentence)\n",
    "    probs = []\n",
    "    for permutation in permutations:\n",
    "        probs.append(estimate_sentence_probability(permutation, trigram_counts, bigram_counts, unigram_counts, fdist, fdist_idx, deltas))\n",
    "    top_n = np.argsort(probs)[::-1][:n]\n",
    "    return [permutations[i] for i in top_n]\n",
    "\n",
    "sentence = ['sino', 'gano', 'me', 'voy', 'a', 'la', 'chingada']\n",
    "print(get_top_n_permutations(sentence, 3, trigram_counts, bigram_counts, unigram_counts, fdist, fdist_idx, deltas))\n",
    "\n",
    "sentence = ['ya', 'se', 'va', 'a', 'acabar', 'la', 'corrupción']\n",
    "print(get_top_n_permutations(sentence, 3, amlo_trigram_counts, amlo_bigram_counts, amlo_unigram_counts, amlo_fdist, amlo_fdist_idx, amlo_deltas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos una funcion para generar permutaciones, con esta creamos todas las permutaciones de una sentencia y obtenemos las sentencias con mayor probabilidad de aparecer en nuestro modelos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tarea fue muy interesante y retadora, ya que aprendi mas sobre el funcionamiento de los ngramas, asi como su eficiencia y limitaciones que pueden llegar a tener, como es el caso del corpus y la memoria que consume, por lo que estos modelos son muy utiles con recursos de memoria superiores a los de un ordenador. No obstante, osn sencillos y generan resultados interesantes con una complejidad sencilla de programacion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
