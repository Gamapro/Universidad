{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import nltk\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "from nltk import ngrams\n",
    "from argparse import Namespace\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import r2_score, accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    lo peor de todo es que no me dan por un tiempo...\n",
       "1    a la vga no seas mamÃ³n 45 putos minutos despuÃ©...\n",
       "2    considero que lo mÃ¡s conveniente seria que lo ...\n",
       "3    el marica de mi ex me tiene bloqueada de todo ...\n",
       "4    mujer despechadaya pinche amlo hazle esta que ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv('./DatasetAgresividad/mex_train.txt', sep='\\r\\n', engine='python', header=None)[0]\n",
    "X_test = pd.read_csv('./DatasetAgresividad/mex_val.txt', sep='\\r\\n', engine='python', header=None)[0]\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramData():\n",
    "\n",
    "    def __init__(self, N: int, vocab_size: int=5000, tokenizer=None, embedding_model=None):\n",
    "        self.N = N\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = tokenizer if tokenizer != None else self.self_tokenizer()\n",
    "        self.punctuations = set([\n",
    "            '.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}','!', '?', 'Â¡', 'Â¿', 'ðŸ’›', 'ðŸ˜¡''ðŸŽµ', '...', 'ðŸ˜°', ''\n",
    "        ])\n",
    "        self.stopwords = set(stopwords.words('spanish'))\n",
    "        self.embedding_model = embedding_model\n",
    "        self.UNK = '<unk>'\n",
    "        self.SOS = '<s>'\n",
    "        self.EOS = '</s>'\n",
    "    \n",
    "    def fit(self, X_train):\n",
    "        self.vocab = self.get_vocab(X_train)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for idx, word in enumerate(self.vocab)}\n",
    "\n",
    "        if self.embedding_model is not None:\n",
    "            self.embedding_matrix = self.get_embedding_matrix()\n",
    "\n",
    "    def get_embedding_matrix(self) -> np.ndarray:\n",
    "        embedding_matrix = np.zeros((self.vocab_size, self.embedding_model.vector_size))\n",
    "        for word in self.vocab:\n",
    "            if word in self.embedding_model:\n",
    "                embedding_matrix[self.word2idx[word]] = self.embedding_model[word]\n",
    "        return embedding_matrix \n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        fdist = FreqDist(\n",
    "            [\n",
    "                word.lower() \n",
    "                for sentence in corpus \n",
    "                    for word in self.tokenizer(sentence) \n",
    "                        if not self.remove_word(word)\n",
    "            ]\n",
    "        )\n",
    "        fdist = dict(fdist)\n",
    "        fdist = sorted(fdist, key=fdist.get, reverse=True)\n",
    "        vocab = fdist[:self.vocab_size]\n",
    "        vocab = set([self.SOS, self.EOS, self.UNK] + vocab)\n",
    "        return set(vocab)\n",
    "    \n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower() \n",
    "        return word.isnumeric() or word in self.punctuations\n",
    "\n",
    "    def self_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(' ')\n",
    "    \n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams, y = [], []\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for word_window in doc_ngram:\n",
    "                word_window_ids = [\n",
    "                    self.word2idx[word] \n",
    "                    for word in word_window]\n",
    "                X_ngrams.append(list(word_window_ids[:-1]))\n",
    "                y.append(word_window_ids[-1])\n",
    "        return np.array(X_ngrams), np.array(y)\n",
    "    \n",
    "    def get_ngram_doc(self, doc: str):\n",
    "        doc_token = self.tokenizer(doc)\n",
    "        doc_token = self.replace_unk(doc_token)\n",
    "        doc_token = [word.lower() for word in doc_token]\n",
    "        doc_token = [self.SOS] * (self.N-1) + doc_token + [self.EOS]\n",
    "        return list(ngrams(doc_token, self.N))\n",
    "    \n",
    "    def replace_unk(self, doc_token: list) -> list:\n",
    "        return [word if word in self.vocab else self.UNK for word in doc_token]\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5003"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Namespace()\n",
    "args.N = 4\n",
    "\n",
    "tk = TweetTokenizer()\n",
    "ngram_data = NGramData(args.N, 5000, tk.tokenize)\n",
    "ngram_data.fit(X_train)\n",
    "\n",
    "len(ngram_data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 616,  616,  616],\n",
       "        [ 616,  616, 2183],\n",
       "        [ 616, 2183, 3387],\n",
       "        ...,\n",
       "        [1648, 1472, 2701],\n",
       "        [1472, 2701, 2337],\n",
       "        [2701, 2337, 1579]]),\n",
       " array([2183, 3387,  380, ..., 2337, 1579, 4981]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_test, y_ngram_test = ngram_data.transform(X_test)   \n",
    "X_ngram_train, y_ngram_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 32\n",
    "args.epochs = 100\n",
    "args.lr = 2.3e-1\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args.num_workers = 0\n",
    "\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.embedding_dim = 30\n",
    "args.hidden_dim = 64\n",
    "args.num_layers = 2\n",
    "args.dropout = 0.25\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "    torch.tensor(y_ngram_train, dtype=torch.int64),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_ngram_test, dtype=torch.int64),\n",
    "    torch.tensor(y_ngram_test, dtype=torch.int64),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        self.args = args\n",
    "        self.window_size = args.N-1\n",
    "        self.embedding_size = args.embedding_dim\n",
    "        self.hidden_size = args.hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=args.vocab_size,\n",
    "            embedding_dim=args.embedding_dim,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(\n",
    "            self.embedding_size * self.window_size,\n",
    "            self.hidden_size,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            self.hidden_size,\n",
    "            args.vocab_size,\n",
    "            bias=False\n",
    "        )\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(-1, self.embedding_size * self.window_size)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    with torch.no_grad():\n",
    "        probs = F.softmax(x.detach(), dim=1)\n",
    "        return torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "def model_eval(data, model, gpu=False):\n",
    "    with torch.no_grad():\n",
    "        preds, targets = [], []\n",
    "        for window_words, labels in data:\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "            output = model(window_words)\n",
    "            pred = predict(output)\n",
    "            target = labels.numpy()\n",
    "            preds.extend(pred)\n",
    "            targets.extend(target)\n",
    "    return accuracy_score(targets, preds)\n",
    "\n",
    "def model_eval_f1(data, model, gpu=False):\n",
    "    with torch.no_grad():\n",
    "        preds, targets = [], []\n",
    "        for window_words, labels in data:\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "            output = model(window_words)\n",
    "            pred = predict(output)\n",
    "            target = labels.numpy()\n",
    "            preds.extend(pred)\n",
    "            targets.extend(target)\n",
    "    return f1_score(targets, preds, average='macro')\n",
    "\n",
    "def save_checkpoint(state, is_best, ckp_path, filename):\n",
    "    torch.save(state, os.path.join(ckp_path, filename))\n",
    "    if is_best:\n",
    "        shutil.copyfile(\n",
    "            os.path.join(ckp_path, filename),\n",
    "            os.path.join(ckp_path, 'model_best.pth.tar')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.patience = 20\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "args.ckp_path = 'checkpoints'\n",
    "args.model_name = 'ngram_lm.pt'\n",
    "args.savedir = 'model'\n",
    "\n",
    "if not os.path.exists(args.ckp_path):\n",
    "    os.makedirs(args.ckp_path)\n",
    "            \n",
    "model = NeuralLM(args)\n",
    "\n",
    "if args.device.type == 'cuda':\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=args.lr_factor,\n",
    "    patience=args.lr_patience,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 100, with best loss 4.517\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(args.ckp_path, args.model_name)):\n",
    "    checkpoint = torch.load(os.path.join(args.ckp_path, args.model_name))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_loss = checkpoint['loss']\n",
    "    print('Loaded model from epoch {0}, with best loss {1:.3f}'.format(\n",
    "        start_epoch, best_loss))\n",
    "else:\n",
    "    start_epoch = 1\n",
    "    best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 6.34s\n",
      "\tTrain Loss: 8.397 | Train Acc: 13.94%\n",
      "Epoch: 02 | Epoch Time: 6.57s\n",
      "\tTrain Loss: 6.564 | Train Acc: 14.92%\n",
      "Epoch: 03 | Epoch Time: 6.80s\n",
      "\tTrain Loss: 6.905 | Train Acc: 15.21%\n",
      "Epoch: 04 | Epoch Time: 7.12s\n",
      "\tTrain Loss: 7.123 | Train Acc: 15.34%\n",
      "Epoch: 05 | Epoch Time: 7.05s\n",
      "\tTrain Loss: 7.103 | Train Acc: 15.36%\n",
      "Epoch 00107: reducing learning rate of group 0 to 3.5938e-03.\n",
      "Epoch: 06 | Epoch Time: 7.18s\n",
      "\tTrain Loss: 5.047 | Train Acc: 15.40%\n",
      "Epoch: 07 | Epoch Time: 6.98s\n",
      "\tTrain Loss: 7.173 | Train Acc: 15.42%\n",
      "Epoch: 08 | Epoch Time: 7.01s\n",
      "\tTrain Loss: 5.729 | Train Acc: 15.45%\n",
      "Epoch: 09 | Epoch Time: 7.02s\n",
      "\tTrain Loss: 6.025 | Train Acc: 15.67%\n",
      "Epoch: 10 | Epoch Time: 6.97s\n",
      "\tTrain Loss: 4.298 | Train Acc: 15.66%\n",
      "Epoch: 11 | Epoch Time: 7.05s\n",
      "\tTrain Loss: 6.750 | Train Acc: 15.75%\n",
      "Epoch: 12 | Epoch Time: 7.01s\n",
      "\tTrain Loss: 6.388 | Train Acc: 15.76%\n",
      "Epoch: 13 | Epoch Time: 7.15s\n",
      "\tTrain Loss: 4.595 | Train Acc: 15.82%\n",
      "Epoch: 14 | Epoch Time: 6.50s\n",
      "\tTrain Loss: 4.757 | Train Acc: 15.78%\n",
      "Epoch: 15 | Epoch Time: 6.91s\n",
      "\tTrain Loss: 5.092 | Train Acc: 15.86%\n",
      "Epoch: 16 | Epoch Time: 6.71s\n",
      "\tTrain Loss: 5.899 | Train Acc: 15.87%\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.7969e-03.\n",
      "Epoch: 17 | Epoch Time: 6.89s\n",
      "\tTrain Loss: 6.232 | Train Acc: 15.94%\n",
      "Epoch: 18 | Epoch Time: 7.09s\n",
      "\tTrain Loss: 6.589 | Train Acc: 15.95%\n",
      "Epoch: 19 | Epoch Time: 7.06s\n",
      "\tTrain Loss: 5.324 | Train Acc: 15.91%\n",
      "Epoch: 20 | Epoch Time: 6.75s\n",
      "\tTrain Loss: 5.882 | Train Acc: 15.96%\n",
      "Epoch: 21 | Epoch Time: 7.04s\n",
      "\tTrain Loss: 5.857 | Train Acc: 15.97%\n",
      "Epoch: 22 | Epoch Time: 7.08s\n",
      "\tTrain Loss: 6.638 | Train Acc: 16.01%\n",
      "Epoch: 23 | Epoch Time: 7.27s\n",
      "\tTrain Loss: 5.976 | Train Acc: 16.02%\n",
      "Epoch: 24 | Epoch Time: 7.05s\n",
      "\tTrain Loss: 6.230 | Train Acc: 16.04%\n",
      "Epoch: 25 | Epoch Time: 7.03s\n",
      "\tTrain Loss: 4.872 | Train Acc: 16.08%\n",
      "Epoch: 26 | Epoch Time: 6.89s\n",
      "\tTrain Loss: 6.999 | Train Acc: 16.09%\n",
      "Epoch: 27 | Epoch Time: 6.03s\n",
      "\tTrain Loss: 3.813 | Train Acc: 16.15%\n",
      "Epoch 00129: reducing learning rate of group 0 to 8.9844e-04.\n",
      "Epoch: 28 | Epoch Time: 6.03s\n",
      "\tTrain Loss: 6.152 | Train Acc: 16.16%\n",
      "Epoch: 29 | Epoch Time: 6.02s\n",
      "\tTrain Loss: 4.312 | Train Acc: 16.13%\n",
      "Epoch: 30 | Epoch Time: 6.01s\n",
      "\tTrain Loss: 4.345 | Train Acc: 16.15%\n",
      "Epoch: 31 | Epoch Time: 6.39s\n",
      "\tTrain Loss: 5.904 | Train Acc: 16.17%\n",
      "Epoch: 32 | Epoch Time: 6.55s\n",
      "\tTrain Loss: 5.983 | Train Acc: 16.16%\n",
      "Epoch: 33 | Epoch Time: 6.59s\n",
      "\tTrain Loss: 5.643 | Train Acc: 16.20%\n",
      "Epoch: 34 | Epoch Time: 6.30s\n",
      "\tTrain Loss: 4.983 | Train Acc: 16.19%\n",
      "Epoch: 35 | Epoch Time: 6.42s\n",
      "\tTrain Loss: 5.474 | Train Acc: 16.21%\n",
      "Epoch: 36 | Epoch Time: 6.01s\n",
      "\tTrain Loss: 5.253 | Train Acc: 16.22%\n",
      "Epoch: 37 | Epoch Time: 5.97s\n",
      "\tTrain Loss: 6.551 | Train Acc: 16.22%\n",
      "Epoch: 38 | Epoch Time: 6.22s\n",
      "\tTrain Loss: 6.353 | Train Acc: 16.23%\n",
      "Epoch 00140: reducing learning rate of group 0 to 4.4922e-04.\n",
      "Epoch: 39 | Epoch Time: 7.27s\n",
      "\tTrain Loss: 4.837 | Train Acc: 16.27%\n",
      "Epoch: 40 | Epoch Time: 6.94s\n",
      "\tTrain Loss: 5.837 | Train Acc: 16.25%\n",
      "Epoch: 41 | Epoch Time: 6.90s\n",
      "\tTrain Loss: 5.987 | Train Acc: 16.26%\n",
      "Epoch: 42 | Epoch Time: 5.79s\n",
      "\tTrain Loss: 6.000 | Train Acc: 16.26%\n",
      "Epoch: 43 | Epoch Time: 5.74s\n",
      "\tTrain Loss: 5.228 | Train Acc: 16.25%\n",
      "Epoch: 44 | Epoch Time: 5.82s\n",
      "\tTrain Loss: 4.758 | Train Acc: 16.25%\n",
      "Epoch: 45 | Epoch Time: 5.89s\n",
      "\tTrain Loss: 4.708 | Train Acc: 16.26%\n",
      "Epoch: 46 | Epoch Time: 6.53s\n",
      "\tTrain Loss: 5.481 | Train Acc: 16.29%\n",
      "Epoch: 47 | Epoch Time: 8.04s\n",
      "\tTrain Loss: 6.297 | Train Acc: 16.28%\n",
      "Epoch: 48 | Epoch Time: 7.98s\n",
      "\tTrain Loss: 6.794 | Train Acc: 16.30%\n",
      "Epoch: 49 | Epoch Time: 8.11s\n",
      "\tTrain Loss: 5.802 | Train Acc: 16.29%\n",
      "Epoch 00151: reducing learning rate of group 0 to 2.2461e-04.\n",
      "Epoch: 50 | Epoch Time: 7.45s\n",
      "\tTrain Loss: 6.735 | Train Acc: 16.29%\n",
      "Epoch: 51 | Epoch Time: 7.51s\n",
      "\tTrain Loss: 5.813 | Train Acc: 16.29%\n",
      "Epoch: 52 | Epoch Time: 7.36s\n",
      "\tTrain Loss: 5.529 | Train Acc: 16.31%\n",
      "Epoch: 53 | Epoch Time: 7.56s\n",
      "\tTrain Loss: 6.004 | Train Acc: 16.30%\n",
      "Epoch: 54 | Epoch Time: 7.47s\n",
      "\tTrain Loss: 4.792 | Train Acc: 16.30%\n",
      "Epoch: 55 | Epoch Time: 7.39s\n",
      "\tTrain Loss: 5.733 | Train Acc: 16.32%\n",
      "Epoch: 56 | Epoch Time: 7.45s\n",
      "\tTrain Loss: 3.456 | Train Acc: 16.30%\n",
      "Epoch: 57 | Epoch Time: 7.41s\n",
      "\tTrain Loss: 5.462 | Train Acc: 16.32%\n",
      "Epoch: 58 | Epoch Time: 7.46s\n",
      "\tTrain Loss: 5.214 | Train Acc: 16.32%\n",
      "Epoch: 59 | Epoch Time: 7.59s\n",
      "\tTrain Loss: 5.373 | Train Acc: 16.32%\n",
      "Epoch: 60 | Epoch Time: 15.87s\n",
      "\tTrain Loss: 5.501 | Train Acc: 16.33%\n",
      "Epoch 00162: reducing learning rate of group 0 to 1.1230e-04.\n",
      "Epoch: 61 | Epoch Time: 9.90s\n",
      "\tTrain Loss: 6.028 | Train Acc: 16.33%\n",
      "Epoch: 62 | Epoch Time: 9.95s\n",
      "\tTrain Loss: 5.083 | Train Acc: 16.34%\n",
      "Epoch: 63 | Epoch Time: 22.51s\n",
      "\tTrain Loss: 4.277 | Train Acc: 16.34%\n",
      "Epoch: 64 | Epoch Time: 15.22s\n",
      "\tTrain Loss: 5.932 | Train Acc: 16.34%\n",
      "Epoch: 65 | Epoch Time: 24.48s\n",
      "\tTrain Loss: 5.819 | Train Acc: 16.34%\n",
      "Epoch: 66 | Epoch Time: 52.51s\n",
      "\tTrain Loss: 6.011 | Train Acc: 16.34%\n",
      "Epoch: 67 | Epoch Time: 63.36s\n",
      "\tTrain Loss: 6.998 | Train Acc: 16.34%\n",
      "Epoch: 68 | Epoch Time: 56.18s\n",
      "\tTrain Loss: 4.421 | Train Acc: 16.34%\n",
      "Epoch: 69 | Epoch Time: 50.24s\n",
      "\tTrain Loss: 5.375 | Train Acc: 16.34%\n",
      "Epoch: 70 | Epoch Time: 90.49s\n",
      "\tTrain Loss: 4.869 | Train Acc: 16.34%\n",
      "Epoch: 71 | Epoch Time: 87.86s\n",
      "\tTrain Loss: 6.552 | Train Acc: 16.34%\n",
      "Epoch 00173: reducing learning rate of group 0 to 5.6152e-05.\n",
      "Epoch: 72 | Epoch Time: 92.39s\n",
      "\tTrain Loss: 6.612 | Train Acc: 16.34%\n",
      "Epoch: 73 | Epoch Time: 11.21s\n",
      "\tTrain Loss: 6.448 | Train Acc: 16.34%\n",
      "Epoch: 74 | Epoch Time: 8.92s\n",
      "\tTrain Loss: 5.829 | Train Acc: 16.35%\n",
      "Epoch: 75 | Epoch Time: 8.26s\n",
      "\tTrain Loss: 6.047 | Train Acc: 16.37%\n",
      "Epoch: 76 | Epoch Time: 8.09s\n",
      "\tTrain Loss: 5.908 | Train Acc: 16.36%\n",
      "Epoch: 77 | Epoch Time: 8.12s\n",
      "\tTrain Loss: 3.406 | Train Acc: 16.36%\n",
      "Epoch: 78 | Epoch Time: 8.03s\n",
      "\tTrain Loss: 3.720 | Train Acc: 16.36%\n",
      "Epoch: 79 | Epoch Time: 8.20s\n",
      "\tTrain Loss: 5.831 | Train Acc: 16.36%\n",
      "Epoch: 80 | Epoch Time: 8.58s\n",
      "\tTrain Loss: 4.856 | Train Acc: 16.36%\n",
      "Epoch: 81 | Epoch Time: 8.30s\n",
      "\tTrain Loss: 4.990 | Train Acc: 16.35%\n",
      "Epoch: 82 | Epoch Time: 8.50s\n",
      "\tTrain Loss: 6.384 | Train Acc: 16.35%\n",
      "Epoch 00184: reducing learning rate of group 0 to 2.8076e-05.\n",
      "Epoch: 83 | Epoch Time: 8.53s\n",
      "\tTrain Loss: 6.029 | Train Acc: 16.36%\n",
      "Epoch: 84 | Epoch Time: 8.12s\n",
      "\tTrain Loss: 5.797 | Train Acc: 16.36%\n",
      "Epoch: 85 | Epoch Time: 8.22s\n",
      "\tTrain Loss: 5.040 | Train Acc: 16.36%\n",
      "Epoch: 86 | Epoch Time: 8.08s\n",
      "\tTrain Loss: 6.359 | Train Acc: 16.36%\n",
      "Epoch: 87 | Epoch Time: 8.05s\n",
      "\tTrain Loss: 5.703 | Train Acc: 16.36%\n",
      "Epoch: 88 | Epoch Time: 8.14s\n",
      "\tTrain Loss: 4.279 | Train Acc: 16.36%\n",
      "Epoch: 89 | Epoch Time: 8.13s\n",
      "\tTrain Loss: 5.007 | Train Acc: 16.36%\n",
      "Epoch: 90 | Epoch Time: 8.07s\n",
      "\tTrain Loss: 5.443 | Train Acc: 16.36%\n",
      "Epoch: 91 | Epoch Time: 8.12s\n",
      "\tTrain Loss: 4.664 | Train Acc: 16.36%\n",
      "Epoch: 92 | Epoch Time: 8.13s\n",
      "\tTrain Loss: 6.202 | Train Acc: 16.37%\n",
      "Epoch: 93 | Epoch Time: 8.07s\n",
      "\tTrain Loss: 5.769 | Train Acc: 16.36%\n",
      "Epoch 00195: reducing learning rate of group 0 to 1.4038e-05.\n",
      "Epoch: 94 | Epoch Time: 8.13s\n",
      "\tTrain Loss: 4.629 | Train Acc: 16.36%\n",
      "Epoch: 95 | Epoch Time: 8.06s\n",
      "\tTrain Loss: 7.505 | Train Acc: 16.36%\n",
      "Epoch: 96 | Epoch Time: 8.05s\n",
      "\tTrain Loss: 6.173 | Train Acc: 16.36%\n",
      "Epoch: 97 | Epoch Time: 8.00s\n",
      "\tTrain Loss: 4.572 | Train Acc: 16.36%\n",
      "Epoch: 98 | Epoch Time: 8.12s\n",
      "\tTrain Loss: 6.692 | Train Acc: 16.36%\n",
      "Epoch: 99 | Epoch Time: 8.72s\n",
      "\tTrain Loss: 4.192 | Train Acc: 16.36%\n",
      "Epoch: 100 | Epoch Time: 8.05s\n",
      "\tTrain Loss: 4.553 | Train Acc: 16.37%\n",
      "Total Training Time: 1228.71s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()    \n",
    "best_acc = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "for epoch in range(args.epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.device.type == 'cuda':\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(window_words)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    train_acc = model_eval(train_loader, model, args.device.type == 'cuda')\n",
    "    train_metric_history.append(train_acc)\n",
    "\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    if train_acc > best_acc:\n",
    "        best_acc = train_acc\n",
    "        is_best = True\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        is_best = False\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= args.patience:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, is_best, args.ckp_path, args.model_name)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_end_time-epoch_start_time:.2f}s')\n",
    "    print(f'\\tTrain Loss: {loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f'Total Training Time: {end_time-start_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.0011575516149752347\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_acc = model_eval_f1(test_loader, model, args.device.type == 'cuda')\n",
    "print(f'Test F1: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comida 21.857433\n",
      "sistemas 20.6578\n",
      "aquÃ­ 16.491972\n",
      "jotos 16.309084\n",
      "obsesionada 15.068522\n",
      "#panamÃ¡ 14.623585\n",
      "profe 13.6933365\n",
      "voces 13.524374\n",
      "tico 13.416697\n",
      "mueras 13.334407\n"
     ]
    }
   ],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    index = ngram_data.word2idx[word]\n",
    "    embedding = embeddings[index]\n",
    "    cosines = np.dot(embeddings, embedding)\n",
    "    idxs = np.argsort(cosines)[::-1][:n]\n",
    "    for idx in idxs:\n",
    "        print(ngram_data.idx2word[idx], cosines[idx])\n",
    "\n",
    "embeddings = model.embedding.weight.data.cpu().numpy()\n",
    "print_closest_words(embeddings, ngram_data, 'comida', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> yo puede ser que <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "def parse_text(text, tokenizer):\n",
    "    all_tokens = [ \n",
    "        w.lower() if w in ngram_data.word2idx else ngram_data.UNK for w in tokenizer(text)\n",
    "    ]   \n",
    "    token_ids = [\n",
    "        ngram_data.word2idx[w.lower()] for w in all_tokens\n",
    "    ]\n",
    "    return all_tokens, token_ids\n",
    "\n",
    "def generate_sentence(model, initial_text, tokenizer):\n",
    "    model.eval()\n",
    "    all_tokens, token_ids = parse_text(initial_text, tokenizer)\n",
    "    for _ in range(100):\n",
    "        window_words = torch.tensor(token_ids[-model.window_size:])\n",
    "        if args.device.type == 'cuda':\n",
    "            window_words = window_words.cuda()\n",
    "        output = model(window_words)\n",
    "        next_id = predict(output)[0]\n",
    "        all_tokens.append(ngram_data.idx2word[next_id])\n",
    "        token_ids.append(next_id)\n",
    "        if all_tokens[-1] == ngram_data.EOS:\n",
    "            break\n",
    "    return all_tokens\n",
    "\n",
    "initial_text = ngram_data.SOS * 3 + 'yo puede ser que'\n",
    "generated_text = generate_sentence(model, initial_text, ngram_data.tokenizer)\n",
    "print(' '.join(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.493666"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    model.eval()\n",
    "    X, y = ngram_data.transform([text])\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.tensor(X).unsqueeze(0)\n",
    "    y = torch.tensor(y).unsqueeze(0)\n",
    "    if args.device.type == 'cuda':\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "    output = model(X)\n",
    "    probs = F.softmax(output, dim=1).cpu().detach().numpy()\n",
    "\n",
    "    return np.sum([\n",
    "        np.log(\n",
    "            probs[i][w]\n",
    "        )\n",
    "        for i, w in enumerate(y.cpu())\n",
    "    ])\n",
    "\n",
    "log_likelihood(model, '<s> <s> <s> estoy comiendo un', ngram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si no gano me voy a la chingada -59.223415\n",
      "si no gano me voy a chingada la -59.223415\n",
      "si no gano me voy la a chingada -59.223415\n",
      "si no gano me voy la chingada a -59.223415\n",
      "si no gano me voy chingada a la -59.223415\n",
      "...\n",
      "chingada la a no si voy gano me -59.223423\n",
      "chingada la a gano voy si no me -59.223423\n",
      "chingada la a gano voy no si me -59.223423\n",
      "chingada la a voy gano si no me -59.223423\n",
      "chingada la a voy gano no si me -59.223423\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = 'si no gano me voy a la chingada'.split(' ')\n",
    "perms = [ ' '.join(perm) for perm in permutations(word_list)]\n",
    "\n",
    "perms.sort(key=lambda x: log_likelihood(model, '<s> <s> <s> ' + x, ngram_data), reverse=True)\n",
    "\n",
    "for perm in perms[:5]:\n",
    "    print(perm, log_likelihood(model, '<s> <s> <s> ' + perm, ngram_data))\n",
    "print('...')\n",
    "for perm in perms[-5:]:\n",
    "    print(perm, log_likelihood(model, '<s> <s> <s> ' + perm, ngram_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
